{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "embedding_dimension=5\n",
    "negative_samples=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_to_word_map={1:\"One\", 2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\" ,6:\"Six\", 7:\"Seven\", 8:\"Eight\", 9:\"Nine\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "for i in range(10000):\n",
    "    rand_odd_ints=np.random.choice(range(1,10,2),3)\n",
    "    sentences.append(\" \".join(digital_to_word_map[r] for r in rand_odd_ints))\n",
    "    rand_even_ints=np.random.choice(range(2,10,2),3)\n",
    "    sentences.append(\" \".join(digital_to_word_map[r] for r in rand_even_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Five Seven One',\n",
       " 'Two Two Two',\n",
       " 'Nine Five Five',\n",
       " 'Six Two Two',\n",
       " 'Three Three Nine',\n",
       " 'Four Two Eight',\n",
       " 'Five One Nine',\n",
       " 'Four Eight Six',\n",
       " 'Five Three Nine',\n",
       " 'Six Six Eight']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index_map={}\n",
    "index=0\n",
    "for sent in sentences:\n",
    "    for word in sent.lower().split():\n",
    "        #print(word)\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word]=index\n",
    "            index+=1\n",
    "#print(word2index_map)            \n",
    "index2word_map={index:word for word,index in word2index_map.items()}\n",
    "#print(index2word_map)\n",
    "vocabulary_size=len(index2word_map)\n",
    "#print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five 0\n",
      "seven 1\n",
      "one 2\n",
      "two 3\n",
      "nine 4\n",
      "six 5\n",
      "three 6\n",
      "four 7\n",
      "eight 8\n"
     ]
    }
   ],
   "source": [
    "for word,index in word2index_map.items():\n",
    "    print(word,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'five',\n",
       " 1: 'seven',\n",
       " 2: 'one',\n",
       " 3: 'two',\n",
       " 4: 'nine',\n",
       " 5: 'six',\n",
       " 6: 'three',\n",
       " 7: 'four',\n",
       " 8: 'eight'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_gram_pairs=[]\n",
    "for sent in sentences:\n",
    "    tokenized_sent = sent.lower().split()\n",
    "    #print(tokenized_sent)\n",
    "    #print(len(tokenized_sent))\n",
    "    for i in range(1, len(tokenized_sent)-1 ):\n",
    "        #print(i)\n",
    "        #print(tokenized_sent[i-1])\n",
    "        #print(tokenized_sent[i+1])\n",
    "        #print(tokenized_sent[i])\n",
    "        word_contex_pair=[[word2index_map[tokenized_sent[i-1]],\n",
    "                           word2index_map[tokenized_sent[i+1]]],\n",
    "                           word2index_map[tokenized_sent[i]]]\n",
    "        #print(word_contex_pair)\n",
    "        skip_gram_pairs.append([word_contex_pair[1],\n",
    "                                word_contex_pair[0][0]])\n",
    "        skip_gram_pairs.append([word_contex_pair[1],\n",
    "                                word_contex_pair[0][1]])\n",
    "        #print(skip_gram_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipgram_batch(batch_size):\n",
    "    instant_indices = list(range(len(skip_gram_pairs)))\n",
    "    np.random.shuffle(instant_indices)\n",
    "    batch=instant_indices[:batch_size]\n",
    "    x=[skip_gram_pairs[i][0] for i in batch]\n",
    "    y=[[skip_gram_pairs[i][1]] for i in batch]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch,y_batch=get_skipgram_batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 8, 6, 3, 8, 6, 1, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [8], [1], [3], [5], [0], [0], [3]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['five', 'eight', 'three', 'two', 'eight', 'three', 'seven', 'two']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index2word_map[word] for word in x_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['five', 'eight', 'seven', 'two', 'six', 'five', 'five', 'two']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index2word_map[word[0]] for word in y_batch ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=tf.placeholder(tf.int32, shape=[batch_size,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_dimension],-1.0,1.0)\n",
    "    ,name='embedding'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed=tf.nn.embedding_lookup(embedding, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nce_weights=tf.Variable(tf.truncated_normal([vocabulary_size, embedding_dimension],\n",
    "                                           stddev=1.0/math.sqrt(embedding_dimension)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nce_biases=tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(\n",
    "    tf.nn.nce_loss(weights=nce_weights, biases=nce_biases, inputs=embed,\n",
    "                  labels=train_labels, num_sampled=negative_samples, \n",
    "                  num_classes=vocabulary_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step=tf.Variable(0, trainable=False)\n",
    "learningRate=tf.train.exponential_decay(learning_rate=0.1,\n",
    "                                       global_step=global_step,\n",
    "                                       decay_steps=1000,\n",
    "                                       decay_rate=0.95,\n",
    "                                       staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step=tf.train.GradientDescentOptimizer(learningRate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.126676\n",
      "3.0278141\n",
      "2.8113775\n",
      "2.7104216\n",
      "2.5553653\n",
      "2.5925653\n",
      "2.5197506\n",
      "2.4882736\n",
      "2.5662475\n",
      "2.5807066\n",
      "WARNING:tensorflow:From <ipython-input-26-c5f0c05be439>:9: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for step in range(1000):\n",
    "        x_batch,y_batch=get_skipgram_batch(batch_size)\n",
    "        sess.run(train_step,feed_dict={train_inputs:x_batch,train_labels:y_batch})\n",
    "        if step % 100 ==0:\n",
    "            loss_val=sess.run(loss,feed_dict={train_inputs:x_batch,train_labels:y_batch})\n",
    "            print(loss_val)\n",
    "    norm=tf.sqrt(tf.reduce_sum(tf.square(embedding),1,keep_dims=True))\n",
    "    normalized_embedding = embedding/norm\n",
    "    normalized_embedding_matrix=sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.423377  , -0.20916885,  0.48683763,  0.73342395, -0.04559429],\n",
       "       [ 0.43623137, -0.43442598,  0.44560573,  0.6186341 ,  0.1992579 ],\n",
       "       [ 0.42939547, -0.28662187,  0.4637446 ,  0.7196009 , -0.02414428],\n",
       "       [-0.7964664 ,  0.00267597,  0.29750478,  0.04865813, -0.5241732 ],\n",
       "       [ 0.17179987, -0.08152475,  0.6624316 ,  0.28748056,  0.6651147 ],\n",
       "       [-0.528824  , -0.63828695,  0.10252123, -0.13135803, -0.53401285],\n",
       "       [ 0.17590399,  0.06784698,  0.41286522,  0.86611265,  0.20939384],\n",
       "       [-0.63646066, -0.4526813 ,  0.4419637 , -0.43826264,  0.05090633],\n",
       "       [-0.87003005,  0.07162157,  0.45384517, -0.15144064, -0.09491292]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index_map[\"one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_word=normalized_embedding_matrix[word2index_map[\"one\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.42939547, -0.28662187,  0.4637446 ,  0.7196009 , -0.02414428],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_digits = np.dot(normalized_embedding_matrix, ref_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9963902 ,  0.95883775,  1.        , -0.15712959,  0.59514856,\n",
       "       -0.07821596,  0.8657497 , -0.25518996, -0.29033226], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_digits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_digits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=np.argsort(cosine_digits )[::-1][1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 6, 4, 5, 3, 7, 8])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'five',\n",
       " 1: 'seven',\n",
       " 2: 'one',\n",
       " 3: 'two',\n",
       " 4: 'nine',\n",
       " 5: 'six',\n",
       " 6: 'three',\n",
       " 7: 'four',\n",
       " 8: 'eight'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five\n",
      "0.9963902\n",
      "  \n",
      "seven\n",
      "0.95883775\n",
      "  \n",
      "three\n",
      "0.8657497\n",
      "  \n",
      "nine\n",
      "0.59514856\n",
      "  \n",
      "six\n",
      "-0.078215964\n",
      "  \n",
      "two\n",
      "-0.15712959\n",
      "  \n",
      "four\n",
      "-0.25518996\n",
      "  \n",
      "eight\n",
      "-0.29033226\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for f in ff:\n",
    "    print(index2word_map[f])\n",
    "    print(cosine_digits[f])\n",
    "    print('  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
