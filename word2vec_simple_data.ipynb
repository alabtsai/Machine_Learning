{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as  np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['king is a strong man', \n",
    "          'queen is a wise woman', \n",
    "          'boy is a young man',\n",
    "          'girl is a young woman',\n",
    "          'prince is a young king',\n",
    "          'princess is a young queen',\n",
    "          'man is strong', \n",
    "          'woman is pretty',\n",
    "          'prince is a boy will be king',\n",
    "          'princess is a girl will be queen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(corpus):\n",
    "    stop_words = ['is', 'a', 'will', 'be']\n",
    "    results = []\n",
    "    for text in corpus:\n",
    "        tmp = text.split(' ')\n",
    "        for stop_word in stop_words:\n",
    "            if stop_word in tmp:\n",
    "                tmp.remove(stop_word)\n",
    "        results.append(\" \".join(tmp))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = remove_stop_words(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king strong man',\n",
       " 'queen wise woman',\n",
       " 'boy young man',\n",
       " 'girl young woman',\n",
       " 'prince young king',\n",
       " 'princess young queen',\n",
       " 'man strong',\n",
       " 'woman pretty',\n",
       " 'prince boy king',\n",
       " 'princess girl queen']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for text in corpus:\n",
    "    #print(text)\n",
    "    for word in text.split(' '):\n",
    "        #print(word)\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king',\n",
       " 'strong',\n",
       " 'man',\n",
       " 'queen',\n",
       " 'wise',\n",
       " 'woman',\n",
       " 'boy',\n",
       " 'young',\n",
       " 'man',\n",
       " 'girl',\n",
       " 'young',\n",
       " 'woman',\n",
       " 'prince',\n",
       " 'young',\n",
       " 'king',\n",
       " 'princess',\n",
       " 'young',\n",
       " 'queen',\n",
       " 'man',\n",
       " 'strong',\n",
       " 'woman',\n",
       " 'pretty',\n",
       " 'prince',\n",
       " 'boy',\n",
       " 'king',\n",
       " 'princess',\n",
       " 'girl',\n",
       " 'queen']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "from collections import Counter\n",
    "utils.safe_mkdir('visualization')\n",
    "file = open(os.path.join('visualization', 'vocab00.tsv'), 'w')\n",
    "\n",
    "count = [('UNK', -1)]\n",
    "count.extend(Counter(words).most_common(140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "('UNK', -1)\n",
      "('young', 4)\n",
      "('king', 3)\n",
      "('man', 3)\n",
      "('queen', 3)\n",
      "('woman', 3)\n",
      "('strong', 2)\n",
      "('boy', 2)\n",
      "('girl', 2)\n",
      "('prince', 2)\n",
      "('princess', 2)\n",
      "('wise', 1)\n",
      "('pretty', 1)\n"
     ]
    }
   ],
   "source": [
    "print(len(count))\n",
    "print(count[0])\n",
    "print(count[1])\n",
    "print(count[2])\n",
    "print(count[3])\n",
    "print(count[4])\n",
    "print(count[5])\n",
    "print(count[6])\n",
    "print(count[7])\n",
    "print(count[8])\n",
    "print(count[9])\n",
    "print(count[10])\n",
    "print(count[11])\n",
    "print(count[12])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK\n",
      "0\n",
      "young\n",
      "1\n",
      "king\n",
      "2\n",
      "man\n",
      "3\n",
      "queen\n",
      "4\n",
      "woman\n",
      "5\n",
      "strong\n",
      "6\n",
      "boy\n",
      "7\n",
      "girl\n",
      "8\n",
      "prince\n",
      "9\n",
      "princess\n",
      "10\n",
      "wise\n",
      "11\n",
      "pretty\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "index=0\n",
    "dictionary = dict()\n",
    "for word, _ in count:\n",
    "        dictionary[word] = index\n",
    "        print(word)\n",
    "        print(index)\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words_to_index(words, dictionary):\n",
    "    \"\"\" Replace each word in the dataset with its index in the dictionary \"\"\"\n",
    "    return [dictionary[word] if word in dictionary else 0 for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_words = convert_words_to_index(words, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 11,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 10,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 12,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 10,\n",
       " 8,\n",
       " 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "SKIP_WINDOW=1\n",
    "def generate_sample():\n",
    "    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n",
    "    for index, center in enumerate(index_words):\n",
    "        context = random.randint(1, SKIP_WINDOW)\n",
    "        # get a random target before the center word\n",
    "        for target in index_words[max(0, index - context): index]:\n",
    "            yield center, target\n",
    "        # get a random target after the center wrod\n",
    "        for target in index_words[index + 1: index + context + 1]:\n",
    "            yield center, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(batch_size):\n",
    "    single_gen = generate_sample()\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros(batch_size)\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(single_gen)\n",
    "        yield center_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen():\n",
    "    yield from batch_gen(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=5\n",
    "dataset = tf.data.Dataset.from_generator(gen, \n",
    "                                (tf.int32, tf.int32), \n",
    "                                (tf.TensorShape([BATCH_SIZE]), tf.TensorShape([BATCH_SIZE])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_initializable_iterator()\n",
    "center_words, target_words = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=12\n",
    "EMBED_SIZE=2\n",
    "embed_matrix = tf.get_variable('embed_matrix', \n",
    "                                        shape=[VOCAB_SIZE, EMBED_SIZE],\n",
    "                                        initializer=tf.random_uniform_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embed : batch_size x embed_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = tf.nn.embedding_lookup(embed_matrix, center_words-1, name='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12,)\n",
      "(1, 12)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "softmax_weight = tf.get_variable('smx_weight', shape=[EMBED_SIZE,VOCAB_SIZE])\n",
    "\n",
    "row=np.zeros(12).astype('float32')\n",
    "print(row.shape)\n",
    "bias=np.array([row])\n",
    "print(bias.shape)\n",
    "print(bias)\n",
    "b = tf.get_variable(name='bias0', initializer=tf.constant(bias) )\n",
    "\n",
    "\n",
    "##  logit :   batch_size x VOCAB_size \n",
    "logit = tf.matmul(embed, softmax_weight) +b \n",
    "# define loss function to be NCE loss function\n",
    "\n",
    "#entropyy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=target_words, name='entropy')\n",
    "\n",
    "#losss = tf.reduce_mean(entropyy, name='loss') # computes the mean over all the examples in the batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropyy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit, labels=target_words-1, name='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(entropyy, name='loss') # computes the mean over all the examples in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larry/tensorflow_1p5/env/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: generator 'batch_gen' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 99:   2.2\n",
      "Average loss at step 199:   2.2\n",
      "Average loss at step 299:   2.2\n",
      "Average loss at step 399:   2.2\n",
      "Average loss at step 499:   2.2\n",
      "Average loss at step 599:   2.2\n",
      "Average loss at step 699:   2.1\n",
      "Average loss at step 799:   2.1\n",
      "Average loss at step 899:   2.1\n",
      "Average loss at step 999:   2.1\n",
      "Average loss at step 1199:   4.2\n",
      "Average loss at step 1299:   2.1\n",
      "Average loss at step 1399:   2.1\n",
      "Average loss at step 1499:   2.1\n",
      "Average loss at step 1599:   2.1\n",
      "Average loss at step 1699:   2.1\n",
      "Average loss at step 1799:   2.0\n",
      "Average loss at step 1899:   2.0\n",
      "Average loss at step 1999:   2.0\n",
      "Average loss at step 2099:   2.0\n",
      "Average loss at step 2299:   4.0\n",
      "Average loss at step 2399:   2.0\n",
      "Average loss at step 2499:   2.0\n",
      "Average loss at step 2599:   2.0\n",
      "Average loss at step 2699:   2.0\n",
      "Average loss at step 2799:   2.0\n",
      "Average loss at step 2899:   1.9\n",
      "Average loss at step 2999:   1.9\n",
      "Average loss at step 3099:   1.9\n",
      "Average loss at step 3199:   1.9\n",
      "Average loss at step 3399:   3.8\n",
      "Average loss at step 3499:   1.9\n",
      "Average loss at step 3599:   1.9\n",
      "Average loss at step 3699:   1.9\n",
      "Average loss at step 3799:   1.9\n",
      "Average loss at step 3899:   1.9\n",
      "Average loss at step 3999:   1.9\n",
      "Average loss at step 4099:   1.9\n",
      "Average loss at step 4199:   1.9\n",
      "Average loss at step 4299:   1.8\n",
      "Average loss at step 4499:   3.6\n",
      "Average loss at step 4599:   1.8\n",
      "Average loss at step 4699:   1.8\n",
      "Average loss at step 4799:   1.8\n",
      "Average loss at step 4899:   1.8\n",
      "Average loss at step 4999:   1.8\n",
      "Average loss at step 5099:   1.8\n",
      "Average loss at step 5199:   1.8\n",
      "Average loss at step 5299:   1.8\n",
      "Average loss at step 5399:   1.8\n",
      "Average loss at step 5599:   3.5\n",
      "Average loss at step 5699:   1.8\n",
      "Average loss at step 5799:   1.8\n",
      "Average loss at step 5899:   1.8\n",
      "Average loss at step 5999:   1.8\n",
      "Average loss at step 6099:   1.8\n",
      "Average loss at step 6199:   1.7\n",
      "Average loss at step 6299:   1.7\n",
      "Average loss at step 6399:   1.7\n",
      "Average loss at step 6499:   1.7\n",
      "Average loss at step 6699:   3.4\n",
      "Average loss at step 6799:   1.7\n",
      "Average loss at step 6899:   1.7\n",
      "Average loss at step 6999:   1.7\n",
      "Average loss at step 7099:   1.7\n",
      "Average loss at step 7199:   1.7\n",
      "Average loss at step 7299:   1.7\n",
      "Average loss at step 7399:   1.7\n",
      "Average loss at step 7499:   1.7\n",
      "Average loss at step 7599:   1.7\n",
      "Average loss at step 7799:   3.4\n",
      "Average loss at step 7899:   1.7\n",
      "Average loss at step 7999:   1.7\n",
      "Average loss at step 8099:   1.7\n",
      "Average loss at step 8199:   1.7\n",
      "Average loss at step 8299:   1.7\n",
      "Average loss at step 8399:   1.7\n",
      "Average loss at step 8499:   1.7\n",
      "Average loss at step 8599:   1.7\n",
      "Average loss at step 8699:   1.7\n",
      "Average loss at step 8899:   3.4\n",
      "Average loss at step 8999:   1.7\n",
      "Average loss at step 9099:   1.7\n",
      "Average loss at step 9199:   1.7\n",
      "Average loss at step 9299:   1.7\n",
      "Average loss at step 9399:   1.7\n",
      "Average loss at step 9499:   1.7\n",
      "Average loss at step 9599:   1.7\n",
      "Average loss at step 9699:   1.7\n",
      "Average loss at step 9799:   1.7\n",
      "Average loss at step 9999:   3.3\n",
      "Average loss at step 10099:   1.7\n",
      "Average loss at step 10199:   1.7\n",
      "Average loss at step 10299:   1.7\n",
      "Average loss at step 10399:   1.7\n",
      "Average loss at step 10499:   1.7\n",
      "Average loss at step 10599:   1.7\n",
      "Average loss at step 10699:   1.7\n",
      "Average loss at step 10799:   1.7\n",
      "Average loss at step 10899:   1.7\n",
      "Average loss at step 11099:   3.3\n",
      "Average loss at step 11199:   1.7\n",
      "Average loss at step 11299:   1.7\n",
      "Average loss at step 11399:   1.7\n",
      "Average loss at step 11499:   1.7\n",
      "Average loss at step 11599:   1.7\n",
      "Average loss at step 11699:   1.7\n",
      "Average loss at step 11799:   1.7\n",
      "Average loss at step 11899:   1.7\n",
      "Average loss at step 11999:   1.7\n",
      "Average loss at step 12199:   3.3\n",
      "Average loss at step 12299:   1.7\n",
      "Average loss at step 12399:   1.7\n",
      "Average loss at step 12499:   1.7\n",
      "Average loss at step 12599:   1.7\n",
      "Average loss at step 12699:   1.7\n",
      "Average loss at step 12799:   1.7\n",
      "Average loss at step 12899:   1.7\n",
      "Average loss at step 12999:   1.7\n",
      "Average loss at step 13099:   1.7\n",
      "Average loss at step 13299:   3.3\n",
      "Average loss at step 13399:   1.7\n",
      "Average loss at step 13499:   1.7\n",
      "Average loss at step 13599:   1.7\n",
      "Average loss at step 13699:   1.7\n",
      "Average loss at step 13799:   1.7\n",
      "Average loss at step 13899:   1.7\n",
      "Average loss at step 13999:   1.7\n",
      "Average loss at step 14099:   1.7\n",
      "Average loss at step 14199:   1.7\n",
      "Average loss at step 14399:   3.3\n",
      "Average loss at step 14499:   1.7\n",
      "Average loss at step 14599:   1.7\n",
      "Average loss at step 14699:   1.7\n",
      "Average loss at step 14799:   1.7\n",
      "Average loss at step 14899:   1.7\n",
      "Average loss at step 14999:   1.7\n",
      "Average loss at step 15099:   1.6\n",
      "Average loss at step 15199:   1.7\n",
      "Average loss at step 15299:   1.6\n",
      "Average loss at step 15499:   3.3\n",
      "Average loss at step 15599:   1.7\n",
      "Average loss at step 15699:   1.7\n",
      "Average loss at step 15799:   1.7\n",
      "Average loss at step 15899:   1.6\n",
      "Average loss at step 15999:   1.7\n",
      "Average loss at step 16099:   1.6\n",
      "Average loss at step 16199:   1.6\n",
      "Average loss at step 16299:   1.6\n",
      "Average loss at step 16399:   1.6\n",
      "Average loss at step 16599:   3.3\n",
      "Average loss at step 16699:   1.6\n",
      "Average loss at step 16799:   1.6\n",
      "Average loss at step 16899:   1.6\n",
      "Average loss at step 16999:   1.6\n",
      "Average loss at step 17099:   1.6\n",
      "Average loss at step 17199:   1.6\n",
      "Average loss at step 17299:   1.6\n",
      "Average loss at step 17399:   1.6\n",
      "Average loss at step 17499:   1.6\n",
      "Average loss at step 17699:   3.3\n",
      "Average loss at step 17799:   1.6\n",
      "Average loss at step 17899:   1.6\n",
      "Average loss at step 17999:   1.6\n",
      "Average loss at step 18099:   1.6\n",
      "Average loss at step 18199:   1.6\n",
      "Average loss at step 18299:   1.6\n",
      "Average loss at step 18399:   1.6\n",
      "Average loss at step 18499:   1.6\n",
      "Average loss at step 18599:   1.6\n",
      "Average loss at step 18799:   3.3\n",
      "Average loss at step 18899:   1.6\n",
      "Average loss at step 18999:   1.6\n",
      "Average loss at step 19099:   1.6\n",
      "Average loss at step 19199:   1.6\n",
      "Average loss at step 19299:   1.6\n",
      "Average loss at step 19399:   1.6\n",
      "Average loss at step 19499:   1.6\n",
      "Average loss at step 19599:   1.6\n",
      "Average loss at step 19699:   1.6\n",
      "Average loss at step 19899:   3.3\n",
      "Average loss at step 19999:   1.6\n",
      "Average loss at step 20099:   1.6\n",
      "Average loss at step 20199:   1.6\n",
      "Average loss at step 20299:   1.6\n",
      "Average loss at step 20399:   1.6\n",
      "Average loss at step 20499:   1.6\n",
      "Average loss at step 20599:   1.6\n",
      "Average loss at step 20699:   1.6\n",
      "Average loss at step 20799:   1.6\n",
      "Average loss at step 20999:   3.2\n",
      "Average loss at step 21099:   1.6\n",
      "Average loss at step 21199:   1.6\n",
      "Average loss at step 21299:   1.6\n",
      "Average loss at step 21399:   1.6\n",
      "Average loss at step 21499:   1.6\n",
      "Average loss at step 21599:   1.6\n",
      "Average loss at step 21699:   1.6\n",
      "Average loss at step 21799:   1.6\n",
      "Average loss at step 21899:   1.6\n",
      "Average loss at step 22099:   3.2\n",
      "Average loss at step 22199:   1.6\n",
      "Average loss at step 22299:   1.6\n",
      "Average loss at step 22399:   1.6\n",
      "Average loss at step 22499:   1.6\n",
      "Average loss at step 22599:   1.6\n",
      "Average loss at step 22699:   1.6\n",
      "Average loss at step 22799:   1.6\n",
      "Average loss at step 22899:   1.6\n",
      "Average loss at step 22999:   1.6\n",
      "Average loss at step 23199:   3.2\n",
      "Average loss at step 23299:   1.6\n",
      "Average loss at step 23399:   1.6\n",
      "Average loss at step 23499:   1.6\n",
      "Average loss at step 23599:   1.6\n",
      "Average loss at step 23699:   1.6\n",
      "Average loss at step 23799:   1.6\n",
      "Average loss at step 23899:   1.6\n",
      "Average loss at step 23999:   1.6\n",
      "Average loss at step 24099:   1.6\n",
      "Average loss at step 24299:   3.2\n",
      "Average loss at step 24399:   1.6\n",
      "Average loss at step 24499:   1.6\n",
      "Average loss at step 24599:   1.6\n",
      "Average loss at step 24699:   1.6\n",
      "Average loss at step 24799:   1.6\n",
      "Average loss at step 24899:   1.6\n",
      "Average loss at step 24999:   1.6\n",
      "Average loss at step 25099:   1.6\n",
      "Average loss at step 25199:   1.6\n",
      "Average loss at step 25399:   3.2\n",
      "Average loss at step 25499:   1.6\n",
      "Average loss at step 25599:   1.6\n",
      "Average loss at step 25699:   1.6\n",
      "Average loss at step 25799:   1.6\n",
      "Average loss at step 25899:   1.6\n",
      "Average loss at step 25999:   1.6\n",
      "Average loss at step 26099:   1.6\n",
      "Average loss at step 26199:   1.6\n",
      "Average loss at step 26299:   1.6\n",
      "Average loss at step 26499:   3.2\n",
      "Average loss at step 26599:   1.6\n",
      "Average loss at step 26699:   1.6\n",
      "Average loss at step 26799:   1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 26899:   1.6\n",
      "Average loss at step 26999:   1.6\n",
      "Average loss at step 27099:   1.6\n",
      "Average loss at step 27199:   1.6\n",
      "Average loss at step 27299:   1.6\n",
      "Average loss at step 27399:   1.6\n",
      "Average loss at step 27599:   3.2\n",
      "Average loss at step 27699:   1.6\n",
      "Average loss at step 27799:   1.6\n",
      "Average loss at step 27899:   1.6\n",
      "Average loss at step 27999:   1.6\n",
      "Average loss at step 28099:   1.6\n",
      "Average loss at step 28199:   1.6\n",
      "Average loss at step 28299:   1.6\n",
      "Average loss at step 28399:   1.6\n",
      "Average loss at step 28499:   1.6\n",
      "Average loss at step 28699:   3.2\n",
      "Average loss at step 28799:   1.6\n",
      "Average loss at step 28899:   1.6\n",
      "Average loss at step 28999:   1.6\n",
      "Average loss at step 29099:   1.6\n",
      "Average loss at step 29199:   1.6\n",
      "Average loss at step 29299:   1.6\n",
      "Average loss at step 29399:   1.6\n",
      "Average loss at step 29499:   1.6\n",
      "Average loss at step 29599:   1.6\n",
      "Average loss at step 29799:   3.2\n",
      "Average loss at step 29899:   1.6\n",
      "Average loss at step 29999:   1.6\n",
      "Average loss at step 30099:   1.6\n",
      "Average loss at step 30199:   1.6\n",
      "Average loss at step 30299:   1.6\n",
      "Average loss at step 30399:   1.6\n",
      "Average loss at step 30499:   1.6\n",
      "Average loss at step 30599:   1.6\n",
      "Average loss at step 30699:   1.6\n",
      "Average loss at step 30899:   3.2\n",
      "Average loss at step 30999:   1.6\n",
      "Average loss at step 31099:   1.6\n",
      "Average loss at step 31199:   1.6\n",
      "Average loss at step 31299:   1.6\n",
      "Average loss at step 31399:   1.6\n",
      "Average loss at step 31499:   1.6\n",
      "Average loss at step 31599:   1.6\n",
      "Average loss at step 31699:   1.6\n",
      "Average loss at step 31799:   1.6\n",
      "Average loss at step 31999:   3.2\n",
      "Average loss at step 32099:   1.6\n",
      "Average loss at step 32199:   1.6\n",
      "Average loss at step 32299:   1.6\n",
      "Average loss at step 32399:   1.6\n",
      "Average loss at step 32499:   1.6\n",
      "Average loss at step 32599:   1.6\n",
      "Average loss at step 32699:   1.6\n",
      "Average loss at step 32799:   1.6\n",
      "Average loss at step 32899:   1.6\n",
      "Average loss at step 33099:   3.2\n",
      "Average loss at step 33199:   1.6\n",
      "Average loss at step 33299:   1.6\n",
      "Average loss at step 33399:   1.6\n",
      "Average loss at step 33499:   1.6\n",
      "Average loss at step 33599:   1.6\n",
      "Average loss at step 33699:   1.6\n",
      "Average loss at step 33799:   1.6\n",
      "Average loss at step 33899:   1.6\n",
      "Average loss at step 33999:   1.6\n",
      "Average loss at step 34199:   3.2\n",
      "Average loss at step 34299:   1.6\n",
      "Average loss at step 34399:   1.6\n",
      "Average loss at step 34499:   1.6\n",
      "Average loss at step 34599:   1.6\n",
      "Average loss at step 34699:   1.6\n",
      "Average loss at step 34799:   1.6\n",
      "Average loss at step 34899:   1.6\n",
      "Average loss at step 34999:   1.6\n",
      "Average loss at step 35099:   1.6\n",
      "Average loss at step 35299:   3.2\n",
      "Average loss at step 35399:   1.6\n",
      "Average loss at step 35499:   1.6\n",
      "Average loss at step 35599:   1.6\n",
      "Average loss at step 35699:   1.6\n",
      "Average loss at step 35799:   1.6\n",
      "Average loss at step 35899:   1.6\n",
      "Average loss at step 35999:   1.6\n",
      "Average loss at step 36099:   1.6\n",
      "Average loss at step 36199:   1.6\n",
      "Average loss at step 36399:   3.2\n",
      "Average loss at step 36499:   1.6\n",
      "Average loss at step 36599:   1.6\n",
      "Average loss at step 36699:   1.6\n",
      "Average loss at step 36799:   1.6\n",
      "Average loss at step 36899:   1.6\n",
      "Average loss at step 36999:   1.6\n",
      "Average loss at step 37099:   1.6\n",
      "Average loss at step 37199:   1.6\n",
      "Average loss at step 37299:   1.6\n",
      "Average loss at step 37499:   3.2\n",
      "Average loss at step 37599:   1.6\n",
      "Average loss at step 37699:   1.6\n",
      "Average loss at step 37799:   1.6\n",
      "Average loss at step 37899:   1.6\n",
      "Average loss at step 37999:   1.6\n",
      "Average loss at step 38099:   1.6\n",
      "Average loss at step 38199:   1.6\n",
      "Average loss at step 38299:   1.6\n",
      "Average loss at step 38399:   1.6\n",
      "Average loss at step 38599:   3.2\n",
      "Average loss at step 38699:   1.6\n",
      "Average loss at step 38799:   1.6\n",
      "Average loss at step 38899:   1.6\n",
      "Average loss at step 38999:   1.6\n",
      "Average loss at step 39099:   1.6\n",
      "Average loss at step 39199:   1.6\n",
      "Average loss at step 39299:   1.6\n",
      "Average loss at step 39399:   1.6\n",
      "Average loss at step 39499:   1.6\n",
      "Average loss at step 39699:   3.2\n",
      "Average loss at step 39799:   1.6\n",
      "Average loss at step 39899:   1.6\n",
      "Average loss at step 39999:   1.6\n",
      "Average loss at step 40099:   1.6\n",
      "Average loss at step 40199:   1.6\n",
      "Average loss at step 40299:   1.6\n",
      "Average loss at step 40399:   1.6\n",
      "Average loss at step 40499:   1.6\n",
      "Average loss at step 40599:   1.6\n",
      "Average loss at step 40799:   3.2\n",
      "Average loss at step 40899:   1.6\n",
      "Average loss at step 40999:   1.6\n",
      "Average loss at step 41099:   1.6\n",
      "Average loss at step 41199:   1.6\n",
      "Average loss at step 41299:   1.6\n",
      "Average loss at step 41399:   1.6\n",
      "Average loss at step 41499:   1.6\n",
      "Average loss at step 41599:   1.6\n",
      "Average loss at step 41699:   1.6\n",
      "Average loss at step 41899:   3.2\n",
      "Average loss at step 41999:   1.6\n",
      "Average loss at step 42099:   1.6\n",
      "Average loss at step 42199:   1.6\n",
      "Average loss at step 42299:   1.6\n",
      "Average loss at step 42399:   1.6\n",
      "Average loss at step 42499:   1.6\n",
      "Average loss at step 42599:   1.6\n",
      "Average loss at step 42699:   1.6\n",
      "Average loss at step 42799:   1.6\n",
      "Average loss at step 42999:   3.2\n",
      "Average loss at step 43099:   1.6\n",
      "Average loss at step 43199:   1.6\n",
      "Average loss at step 43299:   1.6\n",
      "Average loss at step 43399:   1.6\n",
      "Average loss at step 43499:   1.6\n",
      "Average loss at step 43599:   1.6\n",
      "Average loss at step 43699:   1.6\n",
      "Average loss at step 43799:   1.6\n",
      "Average loss at step 43899:   1.6\n",
      "Average loss at step 44099:   3.2\n",
      "Average loss at step 44199:   1.6\n",
      "Average loss at step 44299:   1.6\n",
      "Average loss at step 44399:   1.6\n",
      "Average loss at step 44499:   1.6\n",
      "Average loss at step 44599:   1.6\n",
      "Average loss at step 44699:   1.6\n",
      "Average loss at step 44799:   1.6\n",
      "Average loss at step 44899:   1.6\n",
      "Average loss at step 44999:   1.6\n",
      "Average loss at step 45199:   3.2\n",
      "Average loss at step 45299:   1.6\n",
      "Average loss at step 45399:   1.6\n",
      "Average loss at step 45499:   1.6\n",
      "Average loss at step 45599:   1.6\n",
      "Average loss at step 45699:   1.6\n",
      "Average loss at step 45799:   1.6\n",
      "Average loss at step 45899:   1.6\n",
      "Average loss at step 45999:   1.6\n",
      "Average loss at step 46099:   1.6\n",
      "Average loss at step 46299:   3.2\n",
      "Average loss at step 46399:   1.6\n",
      "Average loss at step 46499:   1.6\n",
      "Average loss at step 46599:   1.6\n",
      "Average loss at step 46699:   1.6\n",
      "Average loss at step 46799:   1.6\n",
      "Average loss at step 46899:   1.6\n",
      "Average loss at step 46999:   1.6\n",
      "Average loss at step 47099:   1.6\n",
      "Average loss at step 47199:   1.6\n",
      "Average loss at step 47399:   3.2\n",
      "Average loss at step 47499:   1.6\n",
      "Average loss at step 47599:   1.6\n",
      "Average loss at step 47699:   1.6\n",
      "Average loss at step 47799:   1.6\n",
      "Average loss at step 47899:   1.6\n",
      "Average loss at step 47999:   1.6\n",
      "Average loss at step 48099:   1.6\n",
      "Average loss at step 48199:   1.6\n",
      "Average loss at step 48299:   1.6\n",
      "Average loss at step 48499:   3.2\n",
      "Average loss at step 48599:   1.6\n",
      "Average loss at step 48699:   1.6\n",
      "Average loss at step 48799:   1.6\n",
      "Average loss at step 48899:   1.6\n",
      "Average loss at step 48999:   1.6\n",
      "Average loss at step 49099:   1.6\n",
      "Average loss at step 49199:   1.6\n",
      "Average loss at step 49299:   1.6\n",
      "Average loss at step 49399:   1.6\n",
      "Average loss at step 49599:   3.2\n",
      "Average loss at step 49699:   1.6\n",
      "Average loss at step 49799:   1.6\n",
      "Average loss at step 49899:   1.6\n",
      "Average loss at step 49999:   1.6\n",
      "Average loss at step 50099:   1.6\n",
      "Average loss at step 50199:   1.6\n",
      "Average loss at step 50299:   1.6\n",
      "Average loss at step 50399:   1.6\n",
      "Average loss at step 50499:   1.6\n",
      "Average loss at step 50699:   3.2\n",
      "Average loss at step 50799:   1.6\n",
      "Average loss at step 50899:   1.6\n",
      "Average loss at step 50999:   1.6\n",
      "Average loss at step 51099:   1.6\n",
      "Average loss at step 51199:   1.6\n",
      "Average loss at step 51299:   1.6\n",
      "Average loss at step 51399:   1.6\n",
      "Average loss at step 51499:   1.6\n",
      "Average loss at step 51599:   1.6\n",
      "Average loss at step 51799:   3.2\n",
      "Average loss at step 51899:   1.6\n",
      "Average loss at step 51999:   1.6\n",
      "Average loss at step 52099:   1.6\n",
      "Average loss at step 52199:   1.6\n",
      "Average loss at step 52299:   1.6\n",
      "Average loss at step 52399:   1.6\n",
      "Average loss at step 52499:   1.6\n",
      "Average loss at step 52599:   1.6\n",
      "Average loss at step 52699:   1.6\n",
      "Average loss at step 52899:   3.2\n",
      "Average loss at step 52999:   1.6\n",
      "Average loss at step 53099:   1.6\n",
      "Average loss at step 53199:   1.6\n",
      "Average loss at step 53299:   1.6\n",
      "Average loss at step 53399:   1.6\n",
      "Average loss at step 53499:   1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 53599:   1.6\n",
      "Average loss at step 53699:   1.6\n",
      "Average loss at step 53799:   1.6\n",
      "Average loss at step 53999:   3.2\n",
      "Average loss at step 54099:   1.6\n",
      "Average loss at step 54199:   1.6\n",
      "Average loss at step 54299:   1.6\n",
      "Average loss at step 54399:   1.6\n",
      "Average loss at step 54499:   1.6\n",
      "Average loss at step 54599:   1.6\n",
      "Average loss at step 54699:   1.6\n",
      "Average loss at step 54799:   1.6\n",
      "Average loss at step 54899:   1.6\n",
      "Average loss at step 55099:   3.2\n",
      "Average loss at step 55199:   1.6\n",
      "Average loss at step 55299:   1.6\n",
      "Average loss at step 55399:   1.6\n",
      "Average loss at step 55499:   1.6\n",
      "Average loss at step 55599:   1.6\n",
      "Average loss at step 55699:   1.6\n",
      "Average loss at step 55799:   1.6\n",
      "Average loss at step 55899:   1.6\n",
      "Average loss at step 55999:   1.6\n",
      "Average loss at step 56199:   3.2\n",
      "Average loss at step 56299:   1.6\n",
      "Average loss at step 56399:   1.6\n",
      "Average loss at step 56499:   1.6\n",
      "Average loss at step 56599:   1.6\n",
      "Average loss at step 56699:   1.6\n",
      "Average loss at step 56799:   1.6\n",
      "Average loss at step 56899:   1.6\n",
      "Average loss at step 56999:   1.6\n",
      "Average loss at step 57099:   1.6\n",
      "Average loss at step 57299:   3.2\n",
      "Average loss at step 57399:   1.6\n",
      "Average loss at step 57499:   1.6\n",
      "Average loss at step 57599:   1.6\n",
      "Average loss at step 57699:   1.6\n",
      "Average loss at step 57799:   1.6\n",
      "Average loss at step 57899:   1.6\n",
      "Average loss at step 57999:   1.6\n",
      "Average loss at step 58099:   1.6\n",
      "Average loss at step 58199:   1.6\n",
      "Average loss at step 58399:   3.2\n",
      "Average loss at step 58499:   1.6\n",
      "Average loss at step 58599:   1.6\n",
      "Average loss at step 58699:   1.6\n",
      "Average loss at step 58799:   1.6\n",
      "Average loss at step 58899:   1.6\n",
      "Average loss at step 58999:   1.6\n",
      "Average loss at step 59099:   1.6\n",
      "Average loss at step 59199:   1.6\n",
      "Average loss at step 59299:   1.6\n",
      "Average loss at step 59499:   3.2\n",
      "Average loss at step 59599:   1.6\n",
      "Average loss at step 59699:   1.6\n",
      "Average loss at step 59799:   1.6\n",
      "Average loss at step 59899:   1.6\n",
      "Average loss at step 59999:   1.6\n",
      "Average loss at step 60099:   1.6\n",
      "Average loss at step 60199:   1.6\n",
      "Average loss at step 60299:   1.6\n",
      "Average loss at step 60399:   1.6\n",
      "Average loss at step 60599:   3.2\n",
      "Average loss at step 60699:   1.6\n",
      "Average loss at step 60799:   1.6\n",
      "Average loss at step 60899:   1.6\n",
      "Average loss at step 60999:   1.6\n",
      "Average loss at step 61099:   1.6\n",
      "Average loss at step 61199:   1.6\n",
      "Average loss at step 61299:   1.6\n",
      "Average loss at step 61399:   1.6\n",
      "Average loss at step 61499:   1.6\n",
      "Average loss at step 61699:   3.2\n",
      "Average loss at step 61799:   1.6\n",
      "Average loss at step 61899:   1.6\n",
      "Average loss at step 61999:   1.6\n",
      "Average loss at step 62099:   1.6\n",
      "Average loss at step 62199:   1.6\n",
      "Average loss at step 62299:   1.6\n",
      "Average loss at step 62399:   1.6\n",
      "Average loss at step 62499:   1.6\n",
      "Average loss at step 62599:   1.6\n",
      "Average loss at step 62799:   3.2\n",
      "Average loss at step 62899:   1.6\n",
      "Average loss at step 62999:   1.6\n",
      "Average loss at step 63099:   1.6\n",
      "Average loss at step 63199:   1.6\n",
      "Average loss at step 63299:   1.6\n",
      "Average loss at step 63399:   1.6\n",
      "Average loss at step 63499:   1.6\n",
      "Average loss at step 63599:   1.6\n",
      "Average loss at step 63699:   1.6\n",
      "Average loss at step 63899:   3.2\n",
      "Average loss at step 63999:   1.6\n",
      "Average loss at step 64099:   1.6\n",
      "Average loss at step 64199:   1.6\n",
      "Average loss at step 64299:   1.6\n",
      "Average loss at step 64399:   1.6\n",
      "Average loss at step 64499:   1.6\n",
      "Average loss at step 64599:   1.6\n",
      "Average loss at step 64699:   1.6\n",
      "Average loss at step 64799:   1.6\n",
      "Average loss at step 64999:   3.2\n",
      "Average loss at step 65099:   1.6\n",
      "Average loss at step 65199:   1.6\n",
      "Average loss at step 65299:   1.6\n",
      "Average loss at step 65399:   1.6\n",
      "Average loss at step 65499:   1.6\n",
      "Average loss at step 65599:   1.6\n",
      "Average loss at step 65699:   1.6\n",
      "Average loss at step 65799:   1.6\n",
      "Average loss at step 65899:   1.6\n",
      "Average loss at step 66099:   3.2\n",
      "Average loss at step 66199:   1.6\n",
      "Average loss at step 66299:   1.6\n",
      "Average loss at step 66399:   1.6\n",
      "Average loss at step 66499:   1.6\n",
      "Average loss at step 66599:   1.6\n",
      "Average loss at step 66699:   1.6\n",
      "Average loss at step 66799:   1.6\n",
      "Average loss at step 66899:   1.6\n",
      "Average loss at step 66999:   1.6\n",
      "Average loss at step 67199:   3.2\n",
      "Average loss at step 67299:   1.6\n",
      "Average loss at step 67399:   1.6\n",
      "Average loss at step 67499:   1.6\n",
      "Average loss at step 67599:   1.6\n",
      "Average loss at step 67699:   1.6\n",
      "Average loss at step 67799:   1.6\n",
      "Average loss at step 67899:   1.6\n",
      "Average loss at step 67999:   1.6\n",
      "Average loss at step 68099:   1.6\n",
      "Average loss at step 68299:   3.2\n",
      "Average loss at step 68399:   1.6\n",
      "Average loss at step 68499:   1.6\n",
      "Average loss at step 68599:   1.6\n",
      "Average loss at step 68699:   1.6\n",
      "Average loss at step 68799:   1.6\n",
      "Average loss at step 68899:   1.6\n",
      "Average loss at step 68999:   1.6\n",
      "Average loss at step 69099:   1.6\n",
      "Average loss at step 69199:   1.6\n",
      "Average loss at step 69399:   3.2\n",
      "Average loss at step 69499:   1.6\n",
      "Average loss at step 69599:   1.6\n",
      "Average loss at step 69699:   1.6\n",
      "Average loss at step 69799:   1.6\n",
      "Average loss at step 69899:   1.6\n",
      "Average loss at step 69999:   1.6\n",
      "Average loss at step 70099:   1.6\n",
      "Average loss at step 70199:   1.6\n",
      "Average loss at step 70299:   1.6\n",
      "Average loss at step 70499:   3.2\n",
      "Average loss at step 70599:   1.6\n",
      "Average loss at step 70699:   1.6\n",
      "Average loss at step 70799:   1.6\n",
      "Average loss at step 70899:   1.6\n",
      "Average loss at step 70999:   1.6\n",
      "Average loss at step 71099:   1.6\n",
      "Average loss at step 71199:   1.6\n",
      "Average loss at step 71299:   1.6\n",
      "Average loss at step 71399:   1.6\n",
      "Average loss at step 71599:   3.2\n",
      "Average loss at step 71699:   1.6\n",
      "Average loss at step 71799:   1.6\n",
      "Average loss at step 71899:   1.6\n",
      "Average loss at step 71999:   1.6\n",
      "Average loss at step 72099:   1.6\n",
      "Average loss at step 72199:   1.6\n",
      "Average loss at step 72299:   1.6\n",
      "Average loss at step 72399:   1.6\n",
      "Average loss at step 72499:   1.6\n",
      "Average loss at step 72699:   3.2\n",
      "Average loss at step 72799:   1.6\n",
      "Average loss at step 72899:   1.6\n",
      "Average loss at step 72999:   1.6\n",
      "Average loss at step 73099:   1.6\n",
      "Average loss at step 73199:   1.6\n",
      "Average loss at step 73299:   1.6\n",
      "Average loss at step 73399:   1.6\n",
      "Average loss at step 73499:   1.6\n",
      "Average loss at step 73599:   1.6\n",
      "Average loss at step 73799:   3.2\n",
      "Average loss at step 73899:   1.6\n",
      "Average loss at step 73999:   1.6\n",
      "Average loss at step 74099:   1.6\n",
      "Average loss at step 74199:   1.6\n",
      "Average loss at step 74299:   1.6\n",
      "Average loss at step 74399:   1.6\n",
      "Average loss at step 74499:   1.6\n",
      "Average loss at step 74599:   1.6\n",
      "Average loss at step 74699:   1.6\n",
      "Average loss at step 74899:   3.2\n",
      "Average loss at step 74999:   1.6\n",
      "Average loss at step 75099:   1.6\n",
      "Average loss at step 75199:   1.6\n",
      "Average loss at step 75299:   1.6\n",
      "Average loss at step 75399:   1.6\n",
      "Average loss at step 75499:   1.6\n",
      "Average loss at step 75599:   1.6\n",
      "Average loss at step 75699:   1.6\n",
      "Average loss at step 75799:   1.6\n",
      "Average loss at step 75999:   3.2\n",
      "Average loss at step 76099:   1.6\n",
      "Average loss at step 76199:   1.6\n",
      "Average loss at step 76299:   1.6\n",
      "Average loss at step 76399:   1.6\n",
      "Average loss at step 76499:   1.6\n",
      "Average loss at step 76599:   1.6\n",
      "Average loss at step 76699:   1.6\n",
      "Average loss at step 76799:   1.6\n",
      "Average loss at step 76899:   1.6\n",
      "Average loss at step 77099:   3.2\n",
      "Average loss at step 77199:   1.6\n",
      "Average loss at step 77299:   1.6\n",
      "Average loss at step 77399:   1.6\n",
      "Average loss at step 77499:   1.6\n",
      "Average loss at step 77599:   1.6\n",
      "Average loss at step 77699:   1.6\n",
      "Average loss at step 77799:   1.6\n",
      "Average loss at step 77899:   1.6\n",
      "Average loss at step 77999:   1.6\n",
      "Average loss at step 78199:   3.2\n",
      "Average loss at step 78299:   1.6\n",
      "Average loss at step 78399:   1.6\n",
      "Average loss at step 78499:   1.6\n",
      "Average loss at step 78599:   1.6\n",
      "Average loss at step 78699:   1.6\n",
      "Average loss at step 78799:   1.6\n",
      "Average loss at step 78899:   1.6\n",
      "Average loss at step 78999:   1.6\n",
      "Average loss at step 79099:   1.6\n",
      "Average loss at step 79299:   3.2\n",
      "Average loss at step 79399:   1.6\n",
      "Average loss at step 79499:   1.6\n",
      "Average loss at step 79599:   1.6\n",
      "Average loss at step 79699:   1.6\n",
      "Average loss at step 79799:   1.6\n",
      "Average loss at step 79899:   1.6\n",
      "Average loss at step 79999:   1.6\n",
      "Average loss at step 80099:   1.6\n",
      "Average loss at step 80199:   1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 80399:   3.2\n",
      "Average loss at step 80499:   1.6\n",
      "Average loss at step 80599:   1.6\n",
      "Average loss at step 80699:   1.6\n",
      "Average loss at step 80799:   1.6\n",
      "Average loss at step 80899:   1.6\n",
      "Average loss at step 80999:   1.6\n",
      "Average loss at step 81099:   1.6\n",
      "Average loss at step 81199:   1.6\n",
      "Average loss at step 81299:   1.6\n",
      "Average loss at step 81499:   3.2\n",
      "Average loss at step 81599:   1.6\n",
      "Average loss at step 81699:   1.6\n",
      "Average loss at step 81799:   1.6\n",
      "Average loss at step 81899:   1.6\n",
      "Average loss at step 81999:   1.6\n",
      "Average loss at step 82099:   1.6\n",
      "Average loss at step 82199:   1.6\n",
      "Average loss at step 82299:   1.6\n",
      "Average loss at step 82399:   1.6\n",
      "Average loss at step 82599:   3.2\n",
      "Average loss at step 82699:   1.6\n",
      "Average loss at step 82799:   1.6\n",
      "Average loss at step 82899:   1.6\n",
      "Average loss at step 82999:   1.6\n",
      "Average loss at step 83099:   1.6\n",
      "Average loss at step 83199:   1.6\n",
      "Average loss at step 83299:   1.6\n",
      "Average loss at step 83399:   1.6\n",
      "Average loss at step 83499:   1.6\n",
      "Average loss at step 83699:   3.2\n",
      "Average loss at step 83799:   1.6\n",
      "Average loss at step 83899:   1.6\n",
      "Average loss at step 83999:   1.6\n",
      "Average loss at step 84099:   1.6\n",
      "Average loss at step 84199:   1.6\n",
      "Average loss at step 84299:   1.6\n",
      "Average loss at step 84399:   1.6\n",
      "Average loss at step 84499:   1.6\n",
      "Average loss at step 84599:   1.6\n",
      "Average loss at step 84799:   3.2\n",
      "Average loss at step 84899:   1.6\n",
      "Average loss at step 84999:   1.6\n",
      "Average loss at step 85099:   1.6\n",
      "Average loss at step 85199:   1.6\n",
      "Average loss at step 85299:   1.6\n",
      "Average loss at step 85399:   1.6\n",
      "Average loss at step 85499:   1.6\n",
      "Average loss at step 85599:   1.6\n",
      "Average loss at step 85699:   1.6\n",
      "Average loss at step 85899:   3.2\n",
      "Average loss at step 85999:   1.6\n",
      "Average loss at step 86099:   1.6\n",
      "Average loss at step 86199:   1.6\n",
      "Average loss at step 86299:   1.6\n",
      "Average loss at step 86399:   1.6\n",
      "Average loss at step 86499:   1.6\n",
      "Average loss at step 86599:   1.6\n",
      "Average loss at step 86699:   1.6\n",
      "Average loss at step 86799:   1.6\n",
      "Average loss at step 86999:   3.2\n",
      "Average loss at step 87099:   1.6\n",
      "Average loss at step 87199:   1.6\n",
      "Average loss at step 87299:   1.6\n",
      "Average loss at step 87399:   1.6\n",
      "Average loss at step 87499:   1.6\n",
      "Average loss at step 87599:   1.6\n",
      "Average loss at step 87699:   1.6\n",
      "Average loss at step 87799:   1.6\n",
      "Average loss at step 87899:   1.6\n",
      "Average loss at step 88099:   3.1\n",
      "Average loss at step 88199:   1.6\n",
      "Average loss at step 88299:   1.6\n",
      "Average loss at step 88399:   1.6\n",
      "Average loss at step 88499:   1.6\n",
      "Average loss at step 88599:   1.6\n",
      "Average loss at step 88699:   1.6\n",
      "Average loss at step 88799:   1.6\n",
      "Average loss at step 88899:   1.6\n",
      "Average loss at step 88999:   1.6\n",
      "Average loss at step 89199:   3.1\n",
      "Average loss at step 89299:   1.6\n",
      "Average loss at step 89399:   1.6\n",
      "Average loss at step 89499:   1.6\n",
      "Average loss at step 89599:   1.6\n",
      "Average loss at step 89699:   1.6\n",
      "Average loss at step 89799:   1.6\n",
      "Average loss at step 89899:   1.6\n",
      "Average loss at step 89999:   1.6\n",
      "Average loss at step 90099:   1.6\n",
      "Average loss at step 90299:   3.1\n",
      "Average loss at step 90399:   1.6\n",
      "Average loss at step 90499:   1.6\n",
      "Average loss at step 90599:   1.6\n",
      "Average loss at step 90699:   1.6\n",
      "Average loss at step 90799:   1.6\n",
      "Average loss at step 90899:   1.6\n",
      "Average loss at step 90999:   1.6\n",
      "Average loss at step 91099:   1.6\n",
      "Average loss at step 91199:   1.6\n",
      "Average loss at step 91399:   3.1\n",
      "Average loss at step 91499:   1.6\n",
      "Average loss at step 91599:   1.6\n",
      "Average loss at step 91699:   1.6\n",
      "Average loss at step 91799:   1.6\n",
      "Average loss at step 91899:   1.6\n",
      "Average loss at step 91999:   1.6\n",
      "Average loss at step 92099:   1.6\n",
      "Average loss at step 92199:   1.6\n",
      "Average loss at step 92299:   1.6\n",
      "Average loss at step 92499:   3.1\n",
      "Average loss at step 92599:   1.6\n",
      "Average loss at step 92699:   1.6\n",
      "Average loss at step 92799:   1.6\n",
      "Average loss at step 92899:   1.6\n",
      "Average loss at step 92999:   1.6\n",
      "Average loss at step 93099:   1.6\n",
      "Average loss at step 93199:   1.6\n",
      "Average loss at step 93299:   1.6\n",
      "Average loss at step 93399:   1.6\n",
      "Average loss at step 93599:   3.1\n",
      "Average loss at step 93699:   1.6\n",
      "Average loss at step 93799:   1.6\n",
      "Average loss at step 93899:   1.6\n",
      "Average loss at step 93999:   1.6\n",
      "Average loss at step 94099:   1.6\n",
      "Average loss at step 94199:   1.6\n",
      "Average loss at step 94299:   1.6\n",
      "Average loss at step 94399:   1.6\n",
      "Average loss at step 94499:   1.6\n",
      "Average loss at step 94699:   3.1\n",
      "Average loss at step 94799:   1.6\n",
      "Average loss at step 94899:   1.6\n",
      "Average loss at step 94999:   1.6\n",
      "Average loss at step 95099:   1.6\n",
      "Average loss at step 95199:   1.6\n",
      "Average loss at step 95299:   1.6\n",
      "Average loss at step 95399:   1.6\n",
      "Average loss at step 95499:   1.6\n",
      "Average loss at step 95599:   1.6\n",
      "Average loss at step 95799:   3.1\n",
      "Average loss at step 95899:   1.6\n",
      "Average loss at step 95999:   1.6\n",
      "Average loss at step 96099:   1.6\n",
      "Average loss at step 96199:   1.6\n",
      "Average loss at step 96299:   1.6\n",
      "Average loss at step 96399:   1.6\n",
      "Average loss at step 96499:   1.6\n",
      "Average loss at step 96599:   1.6\n",
      "Average loss at step 96699:   1.6\n",
      "Average loss at step 96899:   3.1\n",
      "Average loss at step 96999:   1.6\n",
      "Average loss at step 97099:   1.6\n",
      "Average loss at step 97199:   1.6\n",
      "Average loss at step 97299:   1.6\n",
      "Average loss at step 97399:   1.6\n",
      "Average loss at step 97499:   1.6\n",
      "Average loss at step 97599:   1.6\n",
      "Average loss at step 97699:   1.6\n",
      "Average loss at step 97799:   1.6\n",
      "Average loss at step 97999:   3.1\n",
      "Average loss at step 98099:   1.6\n",
      "Average loss at step 98199:   1.6\n",
      "Average loss at step 98299:   1.6\n",
      "Average loss at step 98399:   1.6\n",
      "Average loss at step 98499:   1.6\n",
      "Average loss at step 98599:   1.6\n",
      "Average loss at step 98699:   1.6\n",
      "Average loss at step 98799:   1.6\n",
      "Average loss at step 98899:   1.6\n",
      "Average loss at step 99099:   3.1\n",
      "Average loss at step 99199:   1.6\n",
      "Average loss at step 99299:   1.6\n",
      "Average loss at step 99399:   1.6\n",
      "Average loss at step 99499:   1.6\n",
      "Average loss at step 99599:   1.6\n",
      "Average loss at step 99699:   1.6\n",
      "Average loss at step 99799:   1.6\n",
      "Average loss at step 99899:   1.6\n",
      "Average loss at step 99999:   1.6\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_loss =0.0\n",
    "    for index in range(100000):\n",
    "        try:\n",
    "            #print('index=',index)\n",
    "            loss_batch, _=sess.run([loss, optimizer])\n",
    "            total_loss += loss_batch\n",
    "            if (index+1)%100 == 0:\n",
    "                print('Average loss at step {}: {:5.1f}'.format(index, total_loss / 100))\n",
    "                total_loss=0\n",
    "           # print(tt[1])\n",
    "            #print(sess.run(softmax_weight))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            #print('index=',index)\n",
    "            #print('out of oil')\n",
    "            sess.run(iterator.initializer)\n",
    "    vector=sess.run(embed_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21265112,  1.0527214 ],\n",
       "       [-0.4643219 , -0.63834286],\n",
       "       [-1.1560264 , -0.04065271],\n",
       "       [ 0.5215059 ,  1.4900824 ],\n",
       "       [ 1.0009837 , -3.2584188 ],\n",
       "       [ 2.5295954 ,  1.933384  ],\n",
       "       [ 2.8186824 ,  0.5072213 ],\n",
       "       [ 0.81744975,  1.5400214 ],\n",
       "       [ 1.5952566 , -2.9867413 ],\n",
       "       [ 5.2503257 ,  0.72203636],\n",
       "       [-0.45563054,  1.0023572 ],\n",
       "       [ 1.2676234 ,  0.28571072]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9ae41dd208>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(vector[:,0], vector[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
